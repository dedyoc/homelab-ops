# Incident Report: Total Cluster Isolation & DNS Deadlock

**Date:** December 28, 2025
**Affected Node:** `datayoc` (GMKtec NucBox M5 Plus)
**Severity:** Critical (Total Service Outage)

## 1. Root Cause Analysis (RCA)

A routine kernel module installation (`zenpower3`) triggered a Wi-Fi hardware failure on reboot. While Host connectivity was eventually restored, the Kubernetes cluster entered a **Circular Dependency Deadlock**:

1. **The Trigger:** A reboot caused the Wi-Fi interface (`wlp3s0`) to fail initialization.
2. **The Network Split:** K3s started while the Host had no IP, binding the CNI (Flannel) to a dead route.
3. **The DNS Paradox:** The cluster relies on `adguard` (a pod) for DNS. K3s could not pull the `adguard` image because it had no DNS to resolve `docker.io`.
4. **The Zombie Proxies:** Tailscale sidecars started without an internet route, causing them to fail authentication and become "zombies" (Running but isolated).

---

## 2. Resolution Runbook (Command History)

### Phase 1: Restore Host Connectivity

The host came up without an internet connection. We forced the Wi-Fi interface online.

```bash
# 1. Verify isolation (packet loss)
ping -c 3 8.8.8.8

# 2. Force Wi-Fi connection
nmcli connection up <SSID_NAME>

```

### Phase 2: Break the DNS Deadlock (The "Manual Bypass")

Since AdGuard couldn't start, we temporarily forced the Host to use Google DNS and manually injected the critical images into the local NVMe cache.

```bash
# 1. Stop K3s to prevent loop noise
sudo systemctl stop k3s

# 2. Force Host DNS to Google (Bypass local AdGuard)
echo "nameserver 8.8.8.8" | sudo tee /etc/resolv.conf

# 3. Manually pull critical infrastructure images
# (This allows K3s to start them without network resolution later)
sudo k3s crictl pull tailscale/k8s-operator:stable
sudo k3s crictl pull adguard/adguardhome:latest
sudo k3s crictl pull quay.io/argoproj/argocd:v2.13.2

# 4. Flush stale K3s networking interfaces (Fixes "No Route to Host")
sudo ip link delete cni0
sudo ip link delete flannel.1

# 5. Restart K3s
sudo systemctl start k3s

```

### Phase 3: Restore Application Layer

Once K3s restarted, we had to reset the configurations to stop using the manual Google DNS override and clear application crash loops.

```bash
# 1. Revert Host DNS to Fedora default (systemd-resolved)
sudo systemctl start systemd-resolved

# 2. Kill the stuck AdGuard pod (It restarts immediately using the cached image)
kubectl delete pod -n net-utils -l app=adguard

# 3. Restart CoreDNS to pick up the new AdGuard endpoint
kubectl rollout restart deployment coredns -n kube-system

# 4. Restart ArgoCD to flush stale Redis connections
kubectl rollout restart deployment,statefulset -n argocd

```

### Phase 4: Fix "Zombie" Tailscale Proxies

The Ingress proxies (sidecars) were running but had invalid auth keys because they started when the network was down.

```bash
# Delete all Tailscale proxy pods. 
# The Operator (now healthy) immediately recreated them with valid keys.
kubectl delete pod -n tailscale -l app.kubernetes.io/name=tailscale

```

### Phase 5: Patch Metrics Server (Side Issue)

Metrics server was crashing due to TLS validation errors.

```bash
kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

```

---

## 3. Prevention Strategy (Hardening)

To prevent this from happening on the next power outage or reboot, apply these changes to `homelab-ops`.

### A. Configure CoreDNS Fallback

*Location:* `packages/system/coredns-patch.yaml`
*Goal:* If AdGuard dies, automatically failover to Cloudflare (1.1.1.1).

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
data:
  server.override: |
    forward . <ADGUARD_CLUSTER_IP> 1.1.1.1 {
      force_tcp
      policy sequential
    }

```
